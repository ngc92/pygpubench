"""
EXPLOIT #4: Warmup Detection + Timing Manipulation
====================================================
Severity: HIGH

Strategy:
  The warmup phase always uses args.at(0), calling the kernel repeatedly with
  the same input tensor pointer. We detect warmup by tracking the input data_ptr.

  During warmup: add artificial delay (sleep) to inflate time_estimate.
  This causes actual_calls = ceil(1.0 / time_estimate) to drop to minimum (3).
  Fewer iterations = fewer chances for error detection.

  During benchmark: compute correctly but demonstrate that we detected the switch.

  This exploit doesn't bypass correctness on its own, but it demonstrates that
  warmup detection is trivially easy and can reduce benchmark iterations.

Attack surface: Deterministic warmup behavior (same input pointer every time).
"""
import sys
import time

_call_count = 0
_last_ptr = None
_warmup_detected = False
_benchmark_calls = 0


def kernel(output, inp):
    global _call_count, _last_ptr, _warmup_detected, _benchmark_calls
    _call_count += 1

    current_ptr = inp.data_ptr()
    is_warmup = (current_ptr == _last_ptr) or (_call_count <= 2)
    _last_ptr = current_ptr

    if is_warmup and _call_count <= 50:  # cap to avoid infinite warmup
        # Artificial delay to inflate time_estimate
        import torch
        torch.cuda.synchronize()
        time.sleep(0.4)  # 400ms delay per warmup call
    else:
        if not _warmup_detected:
            _warmup_detected = True
            print(f"[Warmup] Warmup ended after {_call_count} calls, switching to benchmark mode", file=sys.stderr)
        _benchmark_calls += 1

    # Always compute correctly (this exploit only manipulates timing/iterations)
    import torch
    weights = torch.tensor([0.2989, 0.5870, 0.1140], device=inp.device, dtype=inp.dtype)
    torch.sum(inp * weights, dim=-1, out=output)
